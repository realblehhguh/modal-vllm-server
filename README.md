# ğŸš€ Run LLMs on Modal with vLLM!
---
**Made with AI (yes I vibe coded this but dont worry I tested my code)**

[![GitHub stars](https://img.shields.io/github/stars/realblehhguh/modal-vllm-server?style=social)](https://github.com/yourusername/your-repo-name)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.11+-brightgreen.svg)](https://python.org)

A sophisticated vLLM-based OpenAI-compatible API server that automatically selects appropriate GPU configurations and optimizes settings based on the model being served. ğŸ¯
---
## âœ¨ Features

- ğŸ§  **Dynamic GPU Selection**: Automatically chooses the right GPU (T4, A10G, A100, H100) based on model size and type
- âš¡ **Automatic Model Optimization**: Configures vLLM parameters optimally for each model
- ğŸ—œï¸ **Quantization Support**: Handles GPTQ, AWQ, and other quantized models automatically  
- â˜ï¸ **Modal Integration**: Seamless deployment on Modal's serverless GPU infrastructure
- ğŸ“¦ **Model Management**: Easy model downloading and caching
- ğŸ”„ **OpenAI Compatible**: Drop-in replacement for OpenAI API

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

- [Modal](https://modal.com) account and CLI installed ğŸŒ
- Hugging Face account for model access ğŸ¤—
- Python 3.11+ ğŸ

### âš™ï¸ Setup

1. **Install Modal CLI** ğŸ’»:
   ```bash
   pip install modal
   modal setup
   ```

2. **Set up Hugging Face token** ğŸ”‘:
   ```bash
   modal secret create huggingface HF_TOKEN=your_hf_token_here
   ```

3. **Clone and run** ğŸƒâ€â™‚ï¸:
   ```bash
   git clone https://github.com/realblehhguh/modal-vllm-server.git
   cd modal-vllm-server
   modal run vllmserver.py::chat
   ```

## ğŸ’¡ Usage Examples

### ğŸ’¬ Basic Chat Session
```bash
# Use default model (DialoGPT-medium)
modal run vllmserver.py::chat

# Use specific model
MODEL_NAME='TinyLlama/TinyLlama-1.1B-Chat-v1.0' modal run vllmserver.py::chat
```

### â“ Custom Questions
```bash
modal run vllmserver.py::chat --questions "Hello!|What can you do?|Write Python code|Thanks!"
```

### ğŸ“¥ Pre-download Models
```bash
modal run vllmserver.py::download --model-name "hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4"
```

### ğŸ§ª Test Specific Models
```bash
modal run vllmserver.py::test_llama31
```

## ğŸ¤– Supported Models

The system automatically detects and optimizes for:

- ğŸ¦™ **Large Models (7B-70B)**: Llama, Mistral, CodeLlama
- ğŸ—œï¸ **Quantized Models**: GPTQ, AWQ, INT4/INT8 variants
- ğŸ£ **Small Models (1-2B)**: TinyLlama, small chat models
- ğŸ’» **Code Models**: CodeLlama, StarCoder variants

### ğŸ¯ GPU Selection Logic

| Model Size | Quantized | GPU Selected | Memory Utilization |
|------------|-----------|--------------|-------------------|
| 70B | âŒ | H100 ğŸš€ | 90% |
| 70B | âœ… | A100 âš¡ | 85% |
| 7B-13B | âŒ | A100 âš¡ | 85% |
| 7B-13B | âœ… | A10G ğŸ’ª | 80% |
| 3B-6B | Any | A10G ğŸ’ª | 80% |
| 1B-2B | Any | T4 ğŸ”§ | 80% |

## âš™ï¸ Configuration

### ğŸ”§ Environment Variables

- `MODEL_NAME`: Hugging Face model identifier ğŸ¤—
- `HF_TOKEN`: Hugging Face access token (set via Modal secrets) ğŸ”

### ğŸ›ï¸ vLLM Configuration

The system automatically configures:
- `max_model_len`: Context length based on model and GPU ğŸ“
- `tensor_parallel_size`: GPU parallelization ğŸ”€
- `quantization`: Detected from model name ğŸ—œï¸
- `dtype`: Optimal data type for GPU ğŸ“Š
- `gpu_memory_utilization`: Safe memory usage ğŸ’¾

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ‘¤ User Request â”‚â”€â”€â”€â–¶â”‚ ğŸ¯ GPU Selection â”‚â”€â”€â”€â–¶â”‚ ğŸš€ vLLM Serverâ”‚
â”‚                 â”‚    â”‚   Logic          â”‚    â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ ğŸ“¥ Model Downloadâ”‚    â”‚ ğŸ”Œ OpenAI API â”‚
                       â”‚ & Optimization   â”‚    â”‚ Endpoint      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Advanced Features

### ğŸ” Model Integrity Checking
- âœ… Validates essential model files on startup
- ğŸ”„ Automatic re-download if corruption detected
- â¯ï¸ Resume interrupted downloads

### âš¡ Performance Optimizations
- ğŸ§  Prefix caching for repeated patterns
- ğŸ“ Chunked prefill for large contexts
- ğŸ¯ Optimized batch processing
- ğŸ’¾ Memory-efficient tensor parallelization

### ğŸ›¡ï¸ Error Handling
- ğŸ”¬ Comprehensive startup diagnostics
- ğŸ”§ GPU compatibility checking
- ğŸ”„ Graceful fallback options
- ğŸ“Š Detailed error reporting

## ğŸ”Œ API Usage

Once running, the server provides OpenAI-compatible endpoints:

### ğŸ Python Example
```python
import openai

client = openai.OpenAI(
    base_url="http://your-modal-url/v1",
    api_key="vllm"  # Can be any string
)

response = client.chat.completions.create(
    model="your-model-name",
    messages=[{"role": "user", "content": "Hello! ğŸ‘‹"}],
    max_tokens=100
)
```

### ğŸŒ Curl Examples

#### ğŸ’¬ Chat Completions
```bash
# Basic chat completion
curl -X POST "https://your-modal-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer vllm" \
  -d '{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "What is the capital of France?"}
  ],
  "max_tokens": 100,
  "temperature": 0.7
}'
```

#### ğŸ¤– Multi-turn Conversation
```bash
curl -X POST "https://your-modal-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer vllm" \
  -d '{
  "model": "your-model-name",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain quantum computing in simple terms"},
    {"role": "assistant", "content": "Quantum computing uses quantum bits..."},
    {"role": "user", "content": "How is it different from classical computing?"}
  ],
  "max_tokens": 200,
  "temperature": 0.5
}'
```

#### ğŸ’» Code Generation
```bash
curl -X POST "https://your-modal-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer vllm" \
  -d '{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "Write a Python function to calculate fibonacci numbers"}
  ],
  "max_tokens": 300,
  "temperature": 0.2,
  "stop": ["```"]
}'
```

#### ğŸ”„ Streaming Response
```bash
curl -X POST "https://your-modal-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer vllm" \
  -d '{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "Tell me a short story about a robot"}
  ],
  "max_tokens": 150,
  "stream": true
}'
```

#### ğŸ“Š Model Information
```bash
# List available models
curl -H "Authorization: Bearer vllm" \
  "https://your-modal-url/v1/models"
```

#### ğŸ›ï¸ Advanced Parameters
```bash
curl -X POST "https://your-modal-url/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer vllm" \
  -d '{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "Generate creative marketing copy for a coffee shop"}
  ],
  "max_tokens": 250,
  "temperature": 0.8,
  "top_p": 0.9,
  "frequency_penalty": 0.1,
  "presence_penalty": 0.1,
  "stop": ["\n\n"]
}'
```

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch
3. âœ¨ Make your changes
4. ğŸ§ª Add tests if applicable
5. ğŸ“‹ Submit a pull request

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues

**ğŸ’¥ Out of Memory Errors**:
- ğŸ“‰ Try a smaller model or quantized version
- ğŸ“ Reduce `max_model_len` in configuration
- ğŸ†™ Use a larger GPU tier

**ğŸ“¥ Model Download Failures**:
- ğŸ”‘ Check HF_TOKEN permissions
- âœï¸ Verify model name spelling
- ğŸ’¾ Ensure sufficient disk space

**â° Startup Timeouts**:
- â³ Large models take 5-8 minutes to load
- ğŸ” Check GPU availability
- ğŸ“Š Monitor Modal logs for progress

## ğŸ“ˆ Changelog

### v1.0.0 ğŸ‰
- ğŸš€ Initial release with vLLM 0.9.1
- ğŸ¯ Dynamic GPU selection
- âš¡ Automatic model optimization
- ğŸ”Œ OpenAI API compatibility

## ğŸ™ Acknowledgements

This project stands on the shoulders of giants. Special thanks to:

- **[vLLM Team](https://github.com/vllm-project/vllm)** ğŸš€ - For creating the incredible vLLM inference engine that powers this server
- **[Modal Labs](https://modal.com)** â˜ï¸ - For providing the serverless GPU infrastructure that makes this possible
- **[Hugging Face](https://huggingface.co)** ğŸ¤— - For hosting the vast ecosystem of open-source models
- **[OpenAI](https://openai.com)** ğŸ§  - For establishing the API standards that ensure compatibility
- **The Open Source AI Community** ğŸŒŸ - For developing and sharing the amazing models that make this all worthwhile

### ğŸ”§ Technologies Used
- **[Python](https://python.org)** - The backbone language
- **[FastAPI](https://fastapi.tiangolo.com)** - For the robust API framework
- **[PyTorch](https://pytorch.org)** - The deep learning foundation
- **[Transformers](https://github.com/huggingface/transformers)** - For model handling and tokenization

### ğŸ’¡ Inspiration
This project was inspired by the need for a simple, automated way to deploy LLMs without worrying about GPU selection and optimization details. The goal was to make powerful language models as accessible as possible while maintaining production-ready performance.

---

**Built with â¤ï¸ by the community, for the community** ğŸŒ
